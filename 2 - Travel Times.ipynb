{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Transit Travel Times with r5py\n",
    "\n",
    "This notebook is modified based on Acess presentation by Dr Willem to calculate access to opportunities using r5py.\n",
    "\n",
    "This is a generally computationally intensive process, and has been the main technical hurdle to overcome in this process.\n",
    "\n",
    "In this workbook, we are going to generate transit travel times using a relativley new Python library `r5py`. R5py is designed to allow Python users to access the open-source R5 engine, a powerful engine that is the spiritual successort to OpenTripPlanner. You can ready more about r5py via [their documentation](https://r5py.readthedocs.io), or check out [R5's own github page](https://github.com/conveyal/r5).\n",
    "In this workbook we are going to generate transit travel time using the Python library r5py.\n",
    "\n",
    "In our example analysis, we want to answer two questions:\n",
    "- How is the access to hospitals distributed across different populations?\n",
    "- How is the access to child care spaces distributed across different populations?\n",
    "- How does peak-period and evening service change this access for various populations?\n",
    "\n",
    "For this we need to generate *two* travel time matrices. One for a peak period (7-9am) and for an evening period (9-11pm).\n",
    "\n",
    "The beauty of the R5 engine is that it allows us to measure a median peak-period value very easily. To do this, we start our analysis at the beginning of our specified time period and set the duration of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geojson\n",
      "  Downloading geojson-3.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Downloading geojson-3.1.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: geojson\n",
      "Successfully installed geojson-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import datetime\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from geojson import Feature, FeatureCollection, Point\n",
    "\n",
    "# R5py imports\n",
    "from r5py import TransportNetwork, TravelTimeMatrixComputer, TransportMode\n",
    "\n",
    "# AWS\n",
    "import boto3\n",
    "import io\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "\n",
    "# Warnings configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#os.environ['GDAL_DATA'] = os.path.join(f'{os.sep}'.join(sys.executable.split(os.sep)[:-1]), 'Library', 'share', 'gdal')\n",
    "\n",
    "# This sets the amount of memory we are using for R5py calcualtions\n",
    "#sys.argv.append([\"--max-memory\", \"8G\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Input 1**: Load \"Opportunities\" datasets\n",
    "Let's start by loading the appropriate data and setting some key settings for R5py. One little quirk we are going to need is that R5py requires our origin/destination points to be named `id`, not anything else like `dauid`. We'll make that change now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1675\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "## Calgary Data\n",
    "da_centroids = gpd.read_file(\"data/calgary/da_centroids_with_locations.geojson\").rename(columns={\"dauid\":\"id\"}) #1675 rows\n",
    "daycares = gpd.read_file(\"data/calgary/daycare_locations.geojson\") #368 (total: OD = 616400)\n",
    "#hospitals = gpd.read_file(\"data/calgary/hospital_locations.geojson\") # 5 (OD = 8375)\n",
    "\n",
    "## Guadalajara Data\n",
    "#da_centroids = gpd.read_file(\"data/guadalajara/UnAdj_100m_2020.geojson\").rename(columns={\"pointid\":\"id\"})#\n",
    "#dest_po = gpd.read_file(\"data/guadalajara/guadalajara_greenspace_perimeterpoints.geojson\")\n",
    "#dest_po[\"id\"] = range(len(da_centroids)+ 1, len(da_centroids) + len(dest_po) + 1) #adding id column\n",
    "\n",
    "print(len(da_centroids))\n",
    "print(len(daycares))\n",
    "#print(len(dest_po)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Input 2**: Set Up Transport Network\n",
    "\n",
    "To calculate travel times, we need to set up a transport network (which happens to be called a `TransportNetwork` class). The transport network needs both an underlying OpenStreetMap PBF file as well as one or more GTFS feeds. So let's go ahead and set up our transport network, which takes as its first argument the path to our PBF file and as a second argument a list of paths to GTFS files (of which we only have one in Calgary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_transport_network(city_name):\n",
    "    \"\"\"\n",
    "    Create a TransportNetwork object for a given city.  \n",
    "    city_name (str): Name of the city (used for folder naming)\n",
    "    TransportNetwork: A TransportNetwork object for the specified city\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(\"data\", city_name)\n",
    "    \n",
    "    # Find the .osm.pbf file\n",
    "    osm_files = [f for f in os.listdir(base_path) if f.endswith('.osm.pbf')]\n",
    "    if not osm_files:\n",
    "        raise FileNotFoundError(f\"No .osm.pbf file found in {base_path}\")\n",
    "    osm_file = os.path.join(base_path, osm_files[0])\n",
    "    \n",
    "    # Find the GTFS zip file\n",
    "    gtfs_files = [f for f in os.listdir(base_path) if f.endswith('.zip')]\n",
    "    if not gtfs_files:\n",
    "        raise FileNotFoundError(f\"No GTFS zip file found in {base_path}\")\n",
    "    gtfs_file = os.path.join(base_path, gtfs_files[0])\n",
    "    \n",
    "    return TransportNetwork(osm_file, [gtfs_file])\n",
    "\n",
    "\n",
    "# Function call\n",
    "city_name = \"calgary\"  # or \"guadalajara\"\n",
    "transport_network = create_transport_network(city_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will build us a transport network which we can use to compute travel times. So let's go ahead and do that next!\n",
    "\n",
    "In a smaller test, we would create a travel time matrix computer (`TravelTimeMatrixComputer`) which lets us specify a whole bunch of potential parameters, most importantly origins and destinations. Our origins are the DA centroids, and our (first) destination will be the hospital centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Guadalajara example\n",
    "#travel_time_computer = TravelTimeMatrixComputer(\n",
    "#    transport_network,\n",
    "#    origins=da_centroids,\n",
    "#    destinations=dest_po,\n",
    "#    departure=datetime.datetime(2021, 1, 1, 7, 0),\n",
    "#    departure_time_window=datetime.timedelta(minutes=10),\n",
    "#    max_time=datetime.timedelta(minutes=30),\n",
    "#    transport_modes=[TransportMode.TRANSIT, TransportMode.WALK]\n",
    "#)\n",
    "\n",
    "## Calgary example (with daycares as destinations)\n",
    "# travel_time_computer = TravelTimeMatrixComputer(\n",
    "#    transport_network,\n",
    "#    origins=da_centroids,\n",
    "#    destinations=daycares,\n",
    "#    departure=datetime.datetime(2023, 3, 15, 7, 0),\n",
    "#    departure_time_window=datetime.timedelta(hours=2),\n",
    "#    transport_modes=[TransportMode.TRANSIT, TransportMode.WALK]\n",
    "# )\n",
    "\n",
    "# travel_time_computer.compute_travel_times()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Travel Times\n",
    "This will take a little while to run, and we'll write the matrix directly to a file (saved in S3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First S3 related codes!\n",
    "try:\n",
    "    s3_client = boto3.client('s3')\n",
    "    S3_BUCKET = 'wri-cities-sandbox'\n",
    "    S3_PREFIX = 'r5py_sandbox/'\n",
    "    USE_S3 = True\n",
    "except NoCredentialsError:\n",
    "    print(\"AWS credentials not found. Will use local storage instead.\")\n",
    "    USE_S3 = False\n",
    "\n",
    "def check_s3_permissions():\n",
    "    \"\"\"\n",
    "    Check if we have the necessary permissions to access the S3 bucket.\n",
    "    \"\"\"\n",
    "    if not USE_S3:\n",
    "        return False\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
    "        return True\n",
    "    except (ClientError, NoCredentialsError) as e:\n",
    "        print(f\"Error accessing S3 bucket: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_to_s3(df, file_name, mode=\"a\"):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to S3 as a CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if mode == 'a':\n",
    "            try:\n",
    "                obj = s3_client.get_object(Bucket=S3_BUCKET, Key=f\"{S3_PREFIX}{file_name}\")\n",
    "                existing_df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "                df = pd.concat([existing_df, df], ignore_index=True)\n",
    "            except ClientError as e:\n",
    "                if e.response['Error']['Code'] != 'NoSuchKey':\n",
    "                    raise\n",
    "                # If NoSuchKey, we're creating a new file, so just proceed with the current df\n",
    "\n",
    "        csv_buffer = io.StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        s3_client.put_object(\n",
    "            Bucket=S3_BUCKET,\n",
    "            Key=f\"{S3_PREFIX}{file_name}\",\n",
    "            Body=csv_buffer.getvalue())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to S3: {e}. Saving locally instead.\")\n",
    "        save_locally(df, file_name, mode)\n",
    "\n",
    "\n",
    "def save_locally(df, city_name, file_name, mode='a'):\n",
    "    output_dir = os.path.join('data', city_name, 'output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    local_path = os.path.join(output_dir, file_name)\n",
    "    df.to_csv(local_path, mode=mode, header=(mode=='w'), index=False)\n",
    "\n",
    "def save_or_append_data(df, city_name, file_name, mode='a'):\n",
    "    if USE_S3 and check_s3_permissions():\n",
    "        save_to_s3(df, file_name, mode)\n",
    "    else:\n",
    "        save_locally(df, city_name, file_name, mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_batches(geodataframe, batch_size):\n",
    "    return math.ceil(len(geodataframe) / batch_size)\n",
    "\n",
    "def process_batch(c_gdf1, c_gdf2, transport_network):\n",
    "    \"\"\"\"returns a df. \"\"\"\n",
    "    travel_time_computer_c = TravelTimeMatrixComputer(\n",
    "                transport_network,\n",
    "                origins = c_gdf1,\n",
    "                destinations = c_gdf2,\n",
    "                departure = datetime.datetime(2023, 3, 15, 7, 0),  #NEEDS TO MODIFY\n",
    "                departure_time_window = datetime.timedelta(hours=2), #NEEDS TO MODIFY\n",
    "                transport_modes = [TransportMode.TRANSIT, TransportMode.WALK]) #NEEDS TO MODIFY\n",
    "    return travel_time_computer_c.compute_travel_times()  \n",
    "\n",
    "def process_batches(origin, destinations, BS_O, BS_D, base_file_name, transport_network, city_name):\n",
    "    \"\"\"\n",
    "    Process the data in batches and append results to a CSV file in S3.\n",
    "    \"\"\"\n",
    "    # Generate a unique file name for each run\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{base_file_name}_{timestamp}.csv\"\n",
    "    \n",
    "    # Check S3 permissions\n",
    "    if check_s3_permissions():\n",
    "        storage_type = \"S3\"\n",
    "    else:\n",
    "        storage_type = \"local\"\n",
    "        print(\"S3 access not available. Using local storage.\")\n",
    "\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    num_b_o = calculate_num_batches(origin, BS_O)\n",
    "    num_b_d = calculate_num_batches(destinations, BS_D)\n",
    "\n",
    "    # Compute first geodataframe in batches\n",
    "    for i in range(num_b_o):\n",
    "        start_idx_gdf1 = i * BS_O\n",
    "        end_idx_gdf1 = min(start_idx_gdf1 + BS_O, len(origin))\n",
    "        chunk_gdf1 = origin.iloc[start_idx_gdf1:end_idx_gdf1]\n",
    "\n",
    "        # Compute second geodataframe in batches\n",
    "        for j in range(num_b_d):\n",
    "            start_idx_gdf2 = j * BS_D\n",
    "            end_idx_gdf2 = min(start_idx_gdf2 + BS_D, len(destinations))\n",
    "            chunk_gdf2 = destinations.iloc[start_idx_gdf2:end_idx_gdf2]\n",
    "\n",
    "            \n",
    "            # MAJOR compute        \n",
    "            travel_times_df = process_batch(chunk_gdf1, chunk_gdf2, transport_network)\n",
    "\n",
    "            # Append the current batch to the CSV file in S3\n",
    "            save_or_append_data(travel_times_df, city_name, file_name, mode='a')\n",
    "\n",
    "            print(f\"Processed and saved batch: origin {i+1}/{num_b_o}, destination {j+1}/{num_b_d}\")\n",
    "\n",
    "    storage_type = \"S3\" if USE_S3 else \"local\"\n",
    "    print(f\"All batches processed and saved to {storage_type} storage: {file_name}\")\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing S3 bucket: Unable to locate credentials\n",
      "S3 access not available. Using local storage.\n",
      "Error accessing S3 bucket: Unable to locate credentials\n",
      "Processed and saved batch: origin 1/2, destination 1/1\n",
      "Error accessing S3 bucket: Unable to locate credentials\n",
      "Processed and saved batch: origin 2/2, destination 1/1\n",
      "All batches processed and saved to S3 storage: travel_times_20240917_191840.csv\n",
      "FIN!\n",
      "CPU times: total: 3min 41s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Usage\n",
    "city_name = 'calgary'\n",
    "base_file_name = 'travel_times'\n",
    "\n",
    "## Check if this is already called in the above cell. Better to call it above instead of here. This call takes some time.\n",
    "# transport_network = create_transport_network(\"Calgary\") \n",
    "\n",
    "# Define the batch size for processing\n",
    "BS_O = 1000  # Adjust as needed - origin\n",
    "BS_D = 1000     # Adjust as needed - destination\n",
    "\n",
    "\n",
    "# Process batches and get the final DataFrame\n",
    "result = process_batches(da_centroids, daycares, BS_O, BS_D, base_file_name, transport_network, city_name)\n",
    "print(\"FIN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Spatial Index**\n",
    "\n",
    "Can ignore this part for now... :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "def find_dest_h3(orig, dest, h3_res=9, ring_size=3):\n",
    "    # h3_res = 7: ~5.2km^2; h3_res = 8: ~0.74km^2; h3_res = 9: ~0.11km^2\n",
    "    # A k-ring of size k around a central hexagon includes all hexagons that are within k steps from the center.\n",
    "    # When ring_size=1, the k-ring includes: (1) The central hexagon; (2) the six hexagons surrounding the central hex. (total of 7 hexs)\n",
    "    # ring_size=2 means we have about 19 hexs\n",
    "\n",
    "    # Ensure same CRS\n",
    "    orig = orig.to_crs(\"EPSG:4326\")\n",
    "    dest = dest.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Convert origins and destinations to H3 indexes\n",
    "    orig[\"h3_i\"] = orig.apply(lambda i: h3.geo_to_h3(i.geometry.y, i.geometry.x, h3_res), axis=1)\n",
    "    dest[\"h3_i\"] = dest.apply(lambda i: h3.geo_to_h3(i.geometry.y, i.geometry.x, h3_res), axis=1)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    orig_rings = orig.apply(lambda row: set(h3.k_ring(row[\"h3_i\"], ring_size)), axis=1)\n",
    "    \n",
    "    for idx, o_ring in orig_rings.items():\n",
    "        o = orig.loc[idx]\n",
    "        matches = dest[dest[\"h3_i\"].isin(o_ring)]\n",
    "\n",
    "        for _, d in matches.iterrows():\n",
    "            results.append({\n",
    "                \"origin_id\": o[\"id\"],\n",
    "                \"dest_id\": d[\"id\"],\n",
    "                \"o_geometry\": o.geometry,\n",
    "                \"d_geometry\": d.geometry})\n",
    "    \n",
    "    od_pairs = gpd.GeoDataFrame(results, geometry=\"o_geometry\", crs=orig.crs)\n",
    "    od_pairs[\"d_geometry\"] = gpd.GeoSeries(od_pairs[\"d_geometry\"], crs=orig.crs)\n",
    "\n",
    "    grouped_od_pairs = od_pairs.groupby(\"origin_id\").agg({\"o_geometry\": \"first\", \"dest_id\": lambda x: list(x), \"d_geometry\": lambda x: list(x)}).reset_index()\n",
    "    grouped_od_pairs = gpd.GeoDataFrame(grouped_od_pairs, geometry=\"o_geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    return od_pairs, grouped_od_pairs\n",
    "\n",
    "def compute_tt(grouped_od_pairs, transport_network, departure_time, time_window, modes):\n",
    "    \"\"\"\n",
    "    Compute travel times for origin-destination pairs using r5py.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    # Iterate through each origin\n",
    "    for _, row in tqdm(grouped_od_pairs.iterrows(), total=len(grouped_od_pairs), desc=\"Processing origins\"):\n",
    "\n",
    "        # Create GeoDataFrame for the current origin\n",
    "        origin = gpd.GeoDataFrame({\"id\": [row.origin_id], \"geometry\": [row.o_geometry]}, crs=grouped_od_pairs.crs)\n",
    "        \n",
    "        # Create GeoDataFrame for all destinations for this origin\n",
    "        destinations = gpd.GeoDataFrame({\"id\": row.dest_id, \"geometry\": row.d_geometry}, crs=grouped_od_pairs.crs)\n",
    "\n",
    "        travel_time_computer = TravelTimeMatrixComputer(\n",
    "                transport_network,\n",
    "                origins=origin,\n",
    "                destinations=destinations,\n",
    "                departure=departure_time,\n",
    "                departure_time_window=time_window,\n",
    "                transport_modes=modes)\n",
    "\n",
    "        travel_times = travel_time_computer.compute_travel_times()\n",
    "\n",
    "        # Aggregate results for each origin (taking min)\n",
    "        # if not travel_times.empty:\n",
    "        #     min_index = travel_times[\"travel_time\"].idxmin()\n",
    "        #     min_travel_time = travel_times.loc[min_index, \"travel_time\"]\n",
    "        #     min_dest_id = travel_times.loc[min_index, \"to_id\"]\n",
    "        #     min_dest_geometry = destinations.loc[destinations[\"id\"] == min_dest_id, \"geometry\"].iloc[0]\n",
    "\n",
    "        # all_results.append({\"origin_id\": row.origin_id,\"o_geometry\": row.o_geometry,\"min_travel_time\": min_travel_time,\n",
    "        #                 \"min_dest_id\": min_dest_id, \"d_geometry\": min_dest_geometry})\n",
    "        all_results.append(travel_times)\n",
    "           \n",
    "    return pd.concat(all_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OD pairs: 6639\n",
      "     origin_id                   o_geometry                   dest_id  \\\n",
      "0     48060056  POINT (-114.09749 51.13788)   [38, 42, 188, 214, 290]   \n",
      "1     48060057  POINT (-114.09353 51.13976)   [38, 42, 188, 214, 290]   \n",
      "2     48060058  POINT (-114.09522 51.13583)   [38, 42, 188, 214, 290]   \n",
      "3     48060059  POINT (-114.09047 51.13885)   [38, 42, 188, 214, 290]   \n",
      "4     48060060  POINT (-114.08807 51.13915)   [38, 42, 188, 214, 290]   \n",
      "...        ...                          ...                       ...   \n",
      "1538  48062787  POINT (-114.13346 51.07331)  [88, 108, 237, 251, 339]   \n",
      "1539  48062789  POINT (-114.04997 51.11896)           [126, 145, 154]   \n",
      "1540  48062790  POINT (-114.06178 51.12943)                [126, 145]   \n",
      "1541  48062791  POINT (-114.16624 51.04638)        [5, 297, 313, 324]   \n",
      "1542  48062792     POINT (-114.16901 51.04)        [5, 297, 313, 324]   \n",
      "\n",
      "                                             d_geometry  \n",
      "0     [POINT (-114.098894 51.1334222), POINT (-114.0...  \n",
      "1     [POINT (-114.098894 51.1334222), POINT (-114.0...  \n",
      "2     [POINT (-114.098894 51.1334222), POINT (-114.0...  \n",
      "3     [POINT (-114.098894 51.1334222), POINT (-114.0...  \n",
      "4     [POINT (-114.098894 51.1334222), POINT (-114.0...  \n",
      "...                                                 ...  \n",
      "1538  [POINT (-114.145682 51.0776122), POINT (-114.1...  \n",
      "1539  [POINT (-114.0535603 51.1244425), POINT (-114....  \n",
      "1540  [POINT (-114.0535603 51.1244425), POINT (-114....  \n",
      "1541  [POINT (-114.180056395548 51.04729805), POINT ...  \n",
      "1542  [POINT (-114.180056395548 51.04729805), POINT ...  \n",
      "\n",
      "[1543 rows x 4 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:8\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniforge-pypy3\\envs\\r5py-env\\Lib\\site-packages\\r5py\\r5\\transport_network.py:73\u001b[0m, in \u001b[0;36mTransportNetwork.__init__\u001b[1;34m(self, osm_pbf, gtfs)\u001b[0m\n\u001b[0;32m     71\u001b[0m transport_network\u001b[38;5;241m.\u001b[39mtransitLayer \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mconveyal\u001b[38;5;241m.\u001b[39mr5\u001b[38;5;241m.\u001b[39mtransit\u001b[38;5;241m.\u001b[39mTransitLayer()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gtfs_file \u001b[38;5;129;01min\u001b[39;00m gtfs:\n\u001b[1;32m---> 73\u001b[0m     gtfs_feed \u001b[38;5;241m=\u001b[39m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconveyal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgtfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGTFSFeed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadOnlyTempFileFromGtfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgtfs_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     transport_network\u001b[38;5;241m.\u001b[39mtransitLayer\u001b[38;5;241m.\u001b[39mloadFromGtfs(gtfs_feed)\n\u001b[0;32m     75\u001b[0m     gtfs_feed\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Running the functions with Calgary data\n",
    "od_pairs, grouped_od_pairs = find_dest_h3(da_centroids, daycares, h3_res=8, ring_size=1)\n",
    "print(f\"Number of OD pairs: {len(od_pairs)}\")\n",
    "\n",
    "print(grouped_od_pairs)\n",
    "\n",
    "#r5py\n",
    "network = TransportNetwork(\"data/calgary/Calgary.osm.pbf\", [\"data/calgary/cgy-gtfs-2023-03-03.zip\"])\n",
    "departure_time = datetime.datetime(2023, 3, 15, 7, 0)\n",
    "time_window = datetime.timedelta(hours=2)\n",
    "modes = [TransportMode.TRANSIT, TransportMode.WALK]\n",
    "\n",
    "#Compute Travel Time\n",
    "travel_times_df = compute_tt(grouped_od_pairs, network, departure_time, time_window, modes)\n",
    "travel_times_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1128 schools in Guadalajara\n"
     ]
    }
   ],
   "source": [
    "#download schools in Guadalajara\n",
    "def get_schools_in_guadalajara():\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    overpass_query = \"\"\"\n",
    "    [out:json];\n",
    "    area[\"name\"=\"Guadalajara\"][\"admin_level\"=\"6\"]->.searchArea;\n",
    "    (\n",
    "      node[\"amenity\"=\"school\"](area.searchArea);\n",
    "      way[\"amenity\"=\"school\"](area.searchArea);\n",
    "      relation[\"amenity\"=\"school\"](area.searchArea);\n",
    "    );\n",
    "    out center;\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "    data = response.json()\n",
    "\n",
    "    features = []\n",
    "    for element in data['elements']:\n",
    "        if element['type'] == 'node':\n",
    "            lon, lat = element['lon'], element['lat']\n",
    "        elif 'center' in element:\n",
    "            lon, lat = element['center']['lon'], element['center']['lat']\n",
    "        else:\n",
    "            continue  # Skip elements without coordinates\n",
    "        \n",
    "        properties = {\n",
    "            'id': element['id'],\n",
    "            'type': element['type'],\n",
    "            'amenity': 'school'\n",
    "        }\n",
    "        properties.update(element.get('tags', {}))\n",
    "        \n",
    "        feature = Feature(geometry=Point((lon, lat)), properties=properties)\n",
    "        features.append(feature)\n",
    "\n",
    "    feature_collection = FeatureCollection(features)\n",
    "    \n",
    "    with open('guadalajara_schools.geojson', 'w', encoding='utf-8') as f:\n",
    "        json.dump(feature_collection, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Downloaded {len(features)} schools in Guadalajara\")\n",
    "\n",
    "get_schools_in_guadalajara()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r5py-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
